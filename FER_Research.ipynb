{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgV3oN8BWjdk",
        "outputId": "c88efaf8-127c-4d0a-b7ff-f0f5beb8bca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Byyb-XEcW06l",
        "outputId": "66621fb9-848e-41c4-868b-fb845dca7752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['.shortcut-targets-by-id', 'MyDrive', '.Trash-0', '.Encrypted']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print(os.listdir(\"/content/drive\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad0MQUe-XBy_",
        "outputId": "4cfa878c-29d1-4f54-8a7d-f6b9a9702282"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['#dream iit', '13. Indefinite Integration.pdf', 'Untitled document (1).gdoc', 'River Ganga.gdoc', 'Experiment-10 Sundareshwar, Yogesh & Likhith.gslides', 'Experiment - 11 Sundareshwar, Likhith & Yogesh.gslides', 'Colab Notebooks', 'DOC-20240608-WA0008..pdf', 'DOC-20240608-WA0015..pdf', 'CraneDekho_PS08.gslides', 'PCL_Phase-2_Team-2_ASEAS_Report.gdoc', 'PCL_Phase-2_Team-2_ASEAS_PPT.gslides', 'OSH University.gdoc', 'JNNIE PROSPECTUS 2025.gdoc', 'Google AI Studio', 'OSH FEES.gdoc', 'OSH INT MEDICAL FEES Private.gdoc', 'OSH Govt.gdoc', 'AC_GST_Tax_Invoice_1.pdf', 'Untitled document.gdoc', 'Valura.ai Beta Test Report: Platform Validation & Strategic Recommendations.gdoc', 'idea_template.gslides', 'PYTHRONE_ SUSTAINABILITY (1).pdf', 'PYTHRONE_ SUSTAINABILITY.pdf', 'Batch-3 _ASEAS_Jain University.gdoc', 'Untitled spreadsheet.gsheet', 'student_data.gsheet', 'NAIN 2.0 Project Status report.gsheet', 'Consumer Data.gsheet', 'Registration Form.gsheet', 'PYTHRONE_ SUSTAINABILITY.gslides', 'IMG-20250916-WA0006.jpg', 'Category\\tTechnologies   Tools\\nFrontend\\tReact.js,....gsheet', 'i mean make the same vertical table horizontal.gsheet', 'SIH25011_TECHNOPHILES.gdoc', 'JayeshRL - Resume.pdf (1).gdoc', 'Annapoorna (1).pdf', 'LNS Assignment Proposal.gdoc', 'WhatsApp Image 2025-10-28 at 15.34.42_525d615e.jpg', 'Vattigunta_Yogesh_Resume.pdf', 'Employee Data.gsheet', 'Polaris.gsheet', 'Dashboard at the Police.mp4', 'Lets Hear from the Farmers (1).mp4', 'DATASET - To Students-20251116T054147Z-1-001.zip', 'FER_Project', 'Colab Real-Time Emotion Detector', 'Attach ZIP file (should contain all 63 images in one folder - renamed as per the instruction provided in http:  ktiwari.in jain dl project.pdf) (File responses)', 'FaceResearch_Project', 'Yogesh_V_RESUME.gdoc', 'DATASET', 'FaceResearch_Project_Dataset']\n"
          ]
        }
      ],
      "source": [
        "print(os.listdir(\"/content/drive/MyDrive\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xjllw8qcYtRU",
        "outputId": "bac4dd5d-bb06-471b-8e12-33b359e3221c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ZIP SOURCE: /content/drive/MyDrive/Attach ZIP file (should contain all 63 images in one folder - renamed as per the instruction provided in http:  ktiwari.in jain dl project.pdf) (File responses)\n",
            "RAW ZIPS: /content/drive/MyDrive/FaceResearch_Project/raw_zips\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "SHORTCUT_NAME = \"Attach ZIP file (should contain all 63 images in one folder - renamed as per the instruction provided in http:  ktiwari.in jain dl project.pdf) (File responses)\"\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/FaceResearch_Project\"\n",
        "ZIP_SOURCE = f\"/content/drive/MyDrive/{SHORTCUT_NAME}\"\n",
        "RAW_ZIPS = f\"{BASE}/raw_zips\"\n",
        "\n",
        "os.makedirs(RAW_ZIPS, exist_ok=True)\n",
        "\n",
        "print(\"ZIP SOURCE:\", ZIP_SOURCE)\n",
        "print(\"RAW ZIPS:\", RAW_ZIPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "151b8d6b",
        "outputId": "22ed2eb9-13bf-4f56-aa40-9f5814f62dcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents of /content/drive/MyDrive:\n",
            "#dream iit\n",
            "13. Indefinite Integration.pdf\n",
            "Untitled document (1).gdoc\n",
            "River Ganga.gdoc\n",
            "Experiment-10 Sundareshwar, Yogesh & Likhith.gslides\n",
            "Experiment - 11 Sundareshwar, Likhith & Yogesh.gslides\n",
            "Colab Notebooks\n",
            "DOC-20240608-WA0008..pdf\n",
            "DOC-20240608-WA0015..pdf\n",
            "CraneDekho_PS08.gslides\n",
            "PCL_Phase-2_Team-2_ASEAS_Report.gdoc\n",
            "PCL_Phase-2_Team-2_ASEAS_PPT.gslides\n",
            "OSH University.gdoc\n",
            "JNNIE PROSPECTUS 2025.gdoc\n",
            "Google AI Studio\n",
            "OSH FEES.gdoc\n",
            "OSH INT MEDICAL FEES Private.gdoc\n",
            "OSH Govt.gdoc\n",
            "AC_GST_Tax_Invoice_1.pdf\n",
            "Untitled document.gdoc\n",
            "Valura.ai Beta Test Report: Platform Validation & Strategic Recommendations.gdoc\n",
            "idea_template.gslides\n",
            "PYTHRONE_ SUSTAINABILITY (1).pdf\n",
            "PYTHRONE_ SUSTAINABILITY.pdf\n",
            "Batch-3 _ASEAS_Jain University.gdoc\n",
            "Untitled spreadsheet.gsheet\n",
            "student_data.gsheet\n",
            "NAIN 2.0 Project Status report.gsheet\n",
            "Consumer Data.gsheet\n",
            "Registration Form.gsheet\n",
            "PYTHRONE_ SUSTAINABILITY.gslides\n",
            "IMG-20250916-WA0006.jpg\n",
            "Category\tTechnologies   Tools\n",
            "Frontend\tReact.js,....gsheet\n",
            "i mean make the same vertical table horizontal.gsheet\n",
            "SIH25011_TECHNOPHILES.gdoc\n",
            "JayeshRL - Resume.pdf (1).gdoc\n",
            "Annapoorna (1).pdf\n",
            "LNS Assignment Proposal.gdoc\n",
            "WhatsApp Image 2025-10-28 at 15.34.42_525d615e.jpg\n",
            "Vattigunta_Yogesh_Resume.pdf\n",
            "Employee Data.gsheet\n",
            "Polaris.gsheet\n",
            "Dashboard at the Police.mp4\n",
            "Lets Hear from the Farmers (1).mp4\n",
            "DATASET - To Students-20251116T054147Z-1-001.zip\n",
            "FER_Project\n",
            "Colab Real-Time Emotion Detector\n",
            "Attach ZIP file (should contain all 63 images in one folder - renamed as per the instruction provided in http:  ktiwari.in jain dl project.pdf) (File responses)\n",
            "FaceResearch_Project\n",
            "Yogesh_V_RESUME.gdoc\n",
            "DATASET\n",
            "FaceResearch_Project_Dataset\n",
            "\n",
            "--- Please identify the exact name of your ZIP file or shortcut from the list above --- \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(\"Contents of /content/drive/MyDrive:\")\n",
        "my_drive_contents = os.listdir(\"/content/drive/MyDrive\")\n",
        "for item in my_drive_contents:\n",
        "    print(item)\n",
        "\n",
        "print(\"\\n--- Please identify the exact name of your ZIP file or shortcut from the list above --- \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb8177c5",
        "outputId": "88acb594-6146-45ae-da5a-3fe90a7ead2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'/content/drive/MyDrive/Attach ZIP file (should contain all 63 images in one folder - renamed as per the instruction provided in http:  ktiwari.in jain dl project.pdf) (File responses)' is a directory. Copying its contents to '/content/drive/MyDrive/FaceResearch_Project/raw_zips'.\n",
            "Clearing existing contents of '/content/drive/MyDrive/FaceResearch_Project/raw_zips' before copying.\n",
            "Contents copy complete.\n",
            "Images are now in: /content/drive/MyDrive/FaceResearch_Project/raw_zips\n",
            "First 5 image files: ['FACIAL DATASET(23BTRCL096) - ADITHYA S S.zip', 'FACIAL DATASET(23BTRCL013) - DEVANGANA S.zip', '23BTRCL019 - MUHAMMAD SAALIM.zip', 'Photos_Dataset_23BTRCL059 - N R NAGASAMHITA.zip', '23BTRCL024 - UDHAYAN G.zip']\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Ensure the source path exists\n",
        "if not os.path.exists(ZIP_SOURCE):\n",
        "    raise FileNotFoundError(f\"Error: The path '{ZIP_SOURCE}' does not exist. Please ensure the shortcut is correctly named and points to an existing file or directory.\")\n",
        "\n",
        "# Determine if ZIP_SOURCE is a file (a true ZIP) or a directory\n",
        "if os.path.isfile(ZIP_SOURCE):\n",
        "    # Original logic for handling a ZIP file\n",
        "    zip_filename = os.path.basename(ZIP_SOURCE)\n",
        "    DEST_ZIP_PATH = os.path.join(RAW_ZIPS, zip_filename)\n",
        "\n",
        "    print(f\"Copying ZIP file '{ZIP_SOURCE}' to '{DEST_ZIP_PATH}'...\")\n",
        "    shutil.copy(ZIP_SOURCE, DEST_ZIP_PATH)\n",
        "    print(\"Copy complete.\")\n",
        "\n",
        "    print(f\"Extracting '{DEST_ZIP_PATH}' to '{RAW_ZIPS}'...\")\n",
        "    with zipfile.ZipFile(DEST_ZIP_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(RAW_ZIPS)\n",
        "    print(\"Extraction complete.\")\n",
        "\n",
        "    # List contents of the extracted folder (assuming the zip contains a single main folder)\n",
        "    extracted_contents = os.listdir(RAW_ZIPS)\n",
        "    if len(extracted_contents) == 1 and os.path.isdir(os.path.join(RAW_ZIPS, extracted_contents[0])):\n",
        "        IMAGE_DIR = os.path.join(RAW_ZIPS, extracted_contents[0])\n",
        "        print(f\"Images are likely in: {IMAGE_DIR}\")\n",
        "        print(\"First 5 image files:\", os.listdir(IMAGE_DIR)[:5])\n",
        "    else:\n",
        "        IMAGE_DIR = RAW_ZIPS # Fallback if zip extracts directly to RAW_ZIPS\n",
        "        print(f\"Images are likely directly in: {IMAGE_DIR}\")\n",
        "        print(\"First 5 image files:\", os.listdir(IMAGE_DIR)[:5])\n",
        "\n",
        "elif os.path.isdir(ZIP_SOURCE):\n",
        "    # If ZIP_SOURCE is a directory, copy its contents to RAW_ZIPS\n",
        "    print(f\"'{ZIP_SOURCE}' is a directory. Copying its contents to '{RAW_ZIPS}'.\")\n",
        "\n",
        "    # Ensure RAW_ZIPS is clean before copying if it contains old extracted files\n",
        "    if os.path.exists(RAW_ZIPS) and os.listdir(RAW_ZIPS):\n",
        "        print(f\"Clearing existing contents of '{RAW_ZIPS}' before copying.\")\n",
        "        for item in os.listdir(RAW_ZIPS):\n",
        "            item_path = os.path.join(RAW_ZIPS, item)\n",
        "            if os.path.isdir(item_path):\n",
        "                shutil.rmtree(item_path)\n",
        "            else:\n",
        "                os.remove(item_path)\n",
        "\n",
        "    for item_name in os.listdir(ZIP_SOURCE):\n",
        "        src_item_path = os.path.join(ZIP_SOURCE, item_name)\n",
        "        dest_item_path = os.path.join(RAW_ZIPS, item_name)\n",
        "        if os.path.isdir(src_item_path):\n",
        "            shutil.copytree(src_item_path, dest_item_path, dirs_exist_ok=True)\n",
        "        else:\n",
        "            shutil.copy2(src_item_path, dest_item_path)\n",
        "    print(\"Contents copy complete.\")\n",
        "\n",
        "    IMAGE_DIR = RAW_ZIPS # Images are now directly in RAW_ZIPS\n",
        "    print(f\"Images are now in: {IMAGE_DIR}\")\n",
        "    print(\"First 5 image files:\", os.listdir(IMAGE_DIR)[:5])\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Error: '{ZIP_SOURCE}' is neither a file nor a directory. Unexpected type.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuBm7a0ncKNV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "RAW_DATA = \"/content/drive/MyDrive/FaceResearch_Project/raw_data\"\n",
        "os.makedirs(RAW_DATA, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TbufYutcX-V"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "BAD_ZIPS = \"/content/drive/MyDrive/FaceResearch_Project/rejected_data/bad_zips\"\n",
        "os.makedirs(BAD_ZIPS, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NXJimAhkdpr_",
        "outputId": "0543c131-69a1-4959-a1be-3c287cc1a308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valid ZIPs extracted: 166\n",
            "Bad / corrupted ZIPs moved: 7\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "ZIP_DIR = \"/content/drive/MyDrive/FaceResearch_Project/raw_zips\"\n",
        "RAW_DATA = \"/content/drive/MyDrive/FaceResearch_Project/raw_data\"\n",
        "\n",
        "os.makedirs(RAW_DATA, exist_ok=True)\n",
        "\n",
        "good, bad = 0, 0\n",
        "\n",
        "for file in os.listdir(ZIP_DIR):\n",
        "    if file.lower().endswith(\".zip\"):\n",
        "        zip_path = os.path.join(ZIP_DIR, file)\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(RAW_DATA)\n",
        "                good += 1\n",
        "        except zipfile.BadZipFile:\n",
        "            # move bad zip for record\n",
        "            shutil.move(zip_path, os.path.join(BAD_ZIPS, file))\n",
        "            bad += 1\n",
        "\n",
        "print(f\"Valid ZIPs extracted: {good}\")\n",
        "print(f\"Bad / corrupted ZIPs moved: {bad}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Th3Q3ZUeWvO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"Items in raw_data:\")\n",
        "for item in os.listdir(RAW_DATA):\n",
        "    print(item)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iHExywcfa7g"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "NORMALIZED = \"/content/drive/MyDrive/FaceResearch_Project/normalized_data\"\n",
        "os.makedirs(NORMALIZED, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2ChlOWLfhM-"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "RAW_DATA = \"/content/drive/MyDrive/FaceResearch_Project/raw_data\"\n",
        "IMG_EXTS = (\".jpg\", \".jpeg\", \".png\")\n",
        "\n",
        "copied = 0\n",
        "\n",
        "for root, dirs, files in os.walk(RAW_DATA):\n",
        "    for file in files:\n",
        "        if file.lower().endswith(IMG_EXTS):\n",
        "            src = os.path.join(root, file)\n",
        "\n",
        "            # CASE 1: image already inside a folder → use folder name\n",
        "            parent = Path(root).name\n",
        "\n",
        "            # CASE 2: loose images → derive identity from filename\n",
        "            if root == RAW_DATA:\n",
        "                # example: 23BTRCL217-01-AN-01.jpg → 23BTRCL217\n",
        "                parent = file.split(\"-\")[0]\n",
        "\n",
        "            dest_dir = os.path.join(NORMALIZED, parent)\n",
        "            os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "            dst = os.path.join(dest_dir, file)\n",
        "            if not os.path.exists(dst):\n",
        "                shutil.copy(src, dst)\n",
        "                copied += 1\n",
        "\n",
        "print(f\"Normalization complete. Total images copied: {copied}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OflF0C2Ng5FQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/FaceResearch_Project\"\n",
        "\n",
        "REJECT = f\"{BASE}/rejected_data\"\n",
        "REASONS = [\n",
        "    \"corrupt\",\n",
        "    \"no_face\",\n",
        "    \"multiple_faces\",\n",
        "    \"blur\",\n",
        "    \"bad_lighting\"\n",
        "]\n",
        "\n",
        "for r in REASONS:\n",
        "    os.makedirs(f\"{REJECT}/{r}\", exist_ok=True)\n",
        "\n",
        "print(\"Rejection folders ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9HIsbc4iAZf"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "haar = cv2.CascadeClassifier(\n",
        "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puDzmtRbiCvY"
      },
      "outputs": [],
      "source": [
        "import os, shutil, cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "NORMALIZED = \"/content/drive/MyDrive/FaceResearch_Project/normalized_data\"\n",
        "CLEAN = \"/content/drive/MyDrive/FaceResearch_Project/processed_data_fast\"\n",
        "REJECT = \"/content/drive/MyDrive/FaceResearch_Project/rejected_data\"\n",
        "\n",
        "os.makedirs(CLEAN, exist_ok=True)\n",
        "\n",
        "BLUR_THRESHOLD = 80\n",
        "DARK_THRESHOLD = 40\n",
        "BRIGHT_THRESHOLD = 220\n",
        "MAX_IMAGES_PER_ID = 25  # IMPORTANT\n",
        "\n",
        "stats = {\n",
        "    \"total\": 0,\n",
        "    \"kept\": 0,\n",
        "    \"no_face\": 0,\n",
        "    \"blur\": 0,\n",
        "    \"bad_lighting\": 0\n",
        "}\n",
        "\n",
        "for identity in tqdm(os.listdir(NORMALIZED)):\n",
        "    src_dir = os.path.join(NORMALIZED, identity)\n",
        "    if not os.path.isdir(src_dir):\n",
        "        continue\n",
        "\n",
        "    dst_dir = os.path.join(CLEAN, identity)\n",
        "    os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "    kept_for_id = 0\n",
        "\n",
        "    for img_name in os.listdir(src_dir):\n",
        "        if kept_for_id >= MAX_IMAGES_PER_ID:\n",
        "            break\n",
        "\n",
        "        img_path = os.path.join(src_dir, img_name)\n",
        "        stats[\"total\"] += 1\n",
        "\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        faces = haar.detectMultiScale(gray, 1.3, 5)\n",
        "        if len(faces) == 0:\n",
        "            stats[\"no_face\"] += 1\n",
        "            continue\n",
        "\n",
        "        blur_score = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "        if blur_score < BLUR_THRESHOLD:\n",
        "            stats[\"blur\"] += 1\n",
        "            continue\n",
        "\n",
        "        mean_intensity = gray.mean()\n",
        "        if mean_intensity < DARK_THRESHOLD or mean_intensity > BRIGHT_THRESHOLD:\n",
        "            stats[\"bad_lighting\"] += 1\n",
        "            continue\n",
        "\n",
        "        shutil.copy(img_path, os.path.join(dst_dir, img_name))\n",
        "        kept_for_id += 1\n",
        "        stats[\"kept\"] += 1\n",
        "\n",
        "print(\"FAST FILTERING COMPLETE\")\n",
        "print(stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0dKJRsa015y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "CLEAN = \"/content/drive/MyDrive/FaceResearch_Project/processed_data_fast\"\n",
        "\n",
        "identities = os.listdir(CLEAN)\n",
        "print(\"Total identities with kept images:\", len(identities))\n",
        "\n",
        "total_imgs = 0\n",
        "for idn in identities:\n",
        "    total_imgs += len(os.listdir(os.path.join(CLEAN, idn)))\n",
        "\n",
        "print(\"Total KEPT images:\", total_imgs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDWhdSqi1P28"
      },
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "from collections import defaultdict\n",
        "\n",
        "CLEAN = \"/content/drive/MyDrive/FaceResearch_Project/processed_data_fast\"\n",
        "DUP_FN = \"/content/drive/MyDrive/FaceResearch_Project/rejected_data/duplicates_filename\"\n",
        "os.makedirs(DUP_FN, exist_ok=True)\n",
        "\n",
        "MAX_PER_EMOTION = 3\n",
        "removed_fn = 0\n",
        "\n",
        "for identity in os.listdir(CLEAN):\n",
        "    id_path = os.path.join(CLEAN, identity)\n",
        "    if not os.path.isdir(id_path):\n",
        "        continue\n",
        "\n",
        "    groups = defaultdict(list)\n",
        "\n",
        "    for img in os.listdir(id_path):\n",
        "        if img.count(\"-\") < 3:\n",
        "            continue\n",
        "        emotion = img.split(\"-\")[2]   # NE, HA, AN, etc.\n",
        "        groups[emotion].append(img)\n",
        "\n",
        "    for emotion, imgs in groups.items():\n",
        "        imgs = sorted(imgs)  # keep lowest indices\n",
        "        for extra in imgs[MAX_PER_EMOTION:]:\n",
        "            shutil.move(os.path.join(id_path, extra),\n",
        "                        os.path.join(DUP_FN, extra))\n",
        "            removed_fn += 1\n",
        "\n",
        "print(\"Filename-based duplicates removed:\", removed_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUz6ttD01VIx"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "from tqdm import tqdm\n",
        "\n",
        "DUP_EXACT = \"/content/drive/MyDrive/FaceResearch_Project/rejected_data/duplicates_exact\"\n",
        "os.makedirs(DUP_EXACT, exist_ok=True)\n",
        "\n",
        "hash_seen = {}\n",
        "removed_hash = 0\n",
        "\n",
        "def md5_hash(path):\n",
        "    h = hashlib.md5()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "for identity in tqdm(os.listdir(CLEAN)):\n",
        "    id_path = os.path.join(CLEAN, identity)\n",
        "    if not os.path.isdir(id_path):\n",
        "        continue\n",
        "\n",
        "    for img in os.listdir(id_path):\n",
        "        img_path = os.path.join(id_path, img)\n",
        "        try:\n",
        "            h = md5_hash(img_path)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        if h in hash_seen:\n",
        "            shutil.move(img_path, os.path.join(DUP_EXACT, img))\n",
        "            removed_hash += 1\n",
        "        else:\n",
        "            hash_seen[h] = img_path\n",
        "\n",
        "print(\"Exact pixel duplicates removed:\", removed_hash)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2xxz3CR1gFU"
      },
      "outputs": [],
      "source": [
        "final_count = 0\n",
        "for root, dirs, files in os.walk(CLEAN):\n",
        "    for f in files:\n",
        "        if f.lower().endswith((\".jpg\",\".jpeg\",\".png\")):\n",
        "            final_count += 1\n",
        "\n",
        "print(\"FINAL UNIQUE KEPT IMAGES:\", final_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZ_c5c_rO0HH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "CLEAN = \"/content/drive/MyDrive/FaceResearch_Project/processed_data_fast\"\n",
        "\n",
        "rolls = []\n",
        "\n",
        "pattern = re.compile(r\"\\d{2}[A-Z]{3,5}\\d{2,4}\")\n",
        "\n",
        "for identity in os.listdir(CLEAN):\n",
        "    id_path = os.path.join(CLEAN, identity)\n",
        "    if not os.path.isdir(id_path):\n",
        "        continue\n",
        "\n",
        "    for img in os.listdir(id_path):\n",
        "        match = pattern.search(img)\n",
        "        if match:\n",
        "            rolls.append(match.group())\n",
        "\n",
        "roll_counts = Counter(rolls)\n",
        "\n",
        "total_unique_students = len(roll_counts)\n",
        "repeated_students = {r: c for r, c in roll_counts.items() if c > 1}\n",
        "\n",
        "print(\"Total unique students (by roll number):\", total_unique_students)\n",
        "print(\"Students appearing more than once:\", len(repeated_students))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXx3eJHAUTTm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3042c68f"
      },
      "source": [
        "# Task\n",
        "Process and clean the image dataset from the `/content/drive/MyDrive/Attach ZIP file (should contain all 63 images in one folder - renamed as per the instruction provided in http: ktiwari.in jain dl project.pdf) (File responses)` shortcut by creating a new project structure, extracting ZIP files, normalizing images by identity, filtering for faces, blur, and lighting with relaxed criteria (minNeighbors=3, BLUR_THRESHOLD=50, DARK_THRESHOLD=20, BRIGHT_THRESHOLD=235, MAX_IMAGES_PER_ID=50), and removing both filename-based and exact pixel duplicates (keeping max 3 images per emotion per identity). Finally, report the total number of unique, processed images remaining."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "721961cd"
      },
      "source": [
        "## Define New Paths for 'DATASET'\n",
        "\n",
        "### Subtask:\n",
        "Set up new directory paths for the 'DATASET' shortcut processing, such as a new base project folder, raw zips, raw data, normalized data, cleaned data, and rejected data folders. This ensures existing data remains untouched.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb64b308"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires defining new directory paths for the 'DATASET' shortcut processing and creating these directories to ensure proper data organization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dec65ca"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# 1. Define NEW_BASE for the new project directory\n",
        "NEW_BASE = \"/content/drive/MyDrive/FaceResearch_Project_Dataset\"\n",
        "\n",
        "# 2. Define ZIP_SOURCE_DATASET to point to the user-provided shortcut path\n",
        "# This path is already defined as SHORTCUT_NAME in a previous cell and seems to be a directory\n",
        "ZIP_SOURCE_DATASET = f\"/content/drive/MyDrive/{SHORTCUT_NAME}\"\n",
        "\n",
        "# 3. Define RAW_ZIPS_DATASET as a subfolder within NEW_BASE for raw ZIP files\n",
        "RAW_ZIPS_DATASET = f\"{NEW_BASE}/raw_zips\"\n",
        "\n",
        "# 4. Define RAW_DATA_DATASET as a subfolder within NEW_BASE for extracted raw data\n",
        "RAW_DATA_DATASET = f\"{NEW_BASE}/raw_data\"\n",
        "\n",
        "# 5. Define NORMALIZED_DATASET as a subfolder within NEW_BASE for normalized image data\n",
        "NORMALIZED_DATASET = f\"{NEW_BASE}/normalized_data\"\n",
        "\n",
        "# 6. Define CLEAN_DATASET as a subfolder within NEW_BASE for cleaned image data\n",
        "CLEAN_DATASET = f\"{NEW_BASE}/processed_data_dataset\"\n",
        "\n",
        "# 7. Define REJECTED_DATASET as a subfolder within NEW_BASE for rejected images\n",
        "REJECTED_DATASET = f\"{NEW_BASE}/rejected_data\"\n",
        "\n",
        "# 8. Use os.makedirs() to create all these new directories if they don't already exist\n",
        "os.makedirs(NEW_BASE, exist_ok=True)\n",
        "os.makedirs(RAW_ZIPS_DATASET, exist_ok=True)\n",
        "os.makedirs(RAW_DATA_DATASET, exist_ok=True)\n",
        "os.makedirs(NORMALIZED_DATASET, exist_ok=True)\n",
        "os.makedirs(CLEAN_DATASET, exist_ok=True)\n",
        "os.makedirs(REJECTED_DATASET, exist_ok=True)\n",
        "\n",
        "# 9. Print out the defined paths to verify their correctness\n",
        "print(\"New Base Project Directory:\", NEW_BASE)\n",
        "print(\"ZIP Source for Dataset:\", ZIP_SOURCE_DATASET)\n",
        "print(\"Raw Zips Directory for Dataset:\", RAW_ZIPS_DATASET)\n",
        "print(\"Raw Data Directory for Dataset:\", RAW_DATA_DATASET)\n",
        "print(\"Normalized Data Directory for Dataset:\", NORMALIZED_DATASET)\n",
        "print(\"Cleaned Data Directory for Dataset:\", CLEAN_DATASET)\n",
        "print(\"Rejected Data Directory for Dataset:\", REJECTED_DATASET)\n",
        "\n",
        "print(\"All new directories created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fd38c16"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `SHORTCUT_NAME` was not defined in the current cell's scope. I will include the definition of `SHORTCUT_NAME` from a previous cell (`Xjllw8qcYtRU`) to resolve this `NameError`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1be57b90"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define SHORTCUT_NAME to ensure it's available in this cell\n",
        "SHORTCUT_NAME = \"Attach ZIP file (should contain all 63 images in one folder - renamed as per the instruction provided in http:  ktiwari.in jain dl project.pdf) (File responses)\"\n",
        "\n",
        "# 1. Define NEW_BASE for the new project directory\n",
        "NEW_BASE = \"/content/drive/MyDrive/FaceResearch_Project_Dataset\"\n",
        "\n",
        "# 2. Define ZIP_SOURCE_DATASET to point to the user-provided shortcut path\n",
        "ZIP_SOURCE_DATASET = f\"/content/drive/MyDrive/{SHORTCUT_NAME}\"\n",
        "\n",
        "# 3. Define RAW_ZIPS_DATASET as a subfolder within NEW_BASE for raw ZIP files\n",
        "RAW_ZIPS_DATASET = f\"{NEW_BASE}/raw_zips\"\n",
        "\n",
        "# 4. Define RAW_DATA_DATASET as a subfolder within NEW_BASE for extracted raw data\n",
        "RAW_DATA_DATASET = f\"{NEW_BASE}/raw_data\"\n",
        "\n",
        "# 5. Define NORMALIZED_DATASET as a subfolder within NEW_BASE for normalized image data\n",
        "NORMALIZED_DATASET = f\"{NEW_BASE}/normalized_data\"\n",
        "\n",
        "# 6. Define CLEAN_DATASET as a subfolder within NEW_BASE for cleaned image data\n",
        "CLEAN_DATASET = f\"{NEW_BASE}/processed_data_dataset\"\n",
        "\n",
        "# 7. Define REJECTED_DATASET as a subfolder within NEW_BASE for rejected images\n",
        "REJECTED_DATASET = f\"{NEW_BASE}/rejected_data\"\n",
        "\n",
        "# 8. Use os.makedirs() to create all these new directories if they don't already exist\n",
        "os.makedirs(NEW_BASE, exist_ok=True)\n",
        "os.makedirs(RAW_ZIPS_DATASET, exist_ok=True)\n",
        "os.makedirs(RAW_DATA_DATASET, exist_ok=True)\n",
        "os.makedirs(NORMALIZED_DATASET, exist_ok=True)\n",
        "os.makedirs(CLEAN_DATASET, exist_ok=True)\n",
        "os.makedirs(REJECTED_DATASET, exist_ok=True)\n",
        "\n",
        "# 9. Print out the defined paths to verify their correctness\n",
        "print(\"New Base Project Directory:\", NEW_BASE)\n",
        "print(\"ZIP Source for Dataset:\", ZIP_SOURCE_DATASET)\n",
        "print(\"Raw Zips Directory for Dataset:\", RAW_ZIPS_DATASET)\n",
        "print(\"Raw Data Directory for Dataset:\", RAW_DATA_DATASET)\n",
        "print(\"Normalized Data Directory for Dataset:\", NORMALIZED_DATASET)\n",
        "print(\"Cleaned Data Directory for Dataset:\", CLEAN_DATASET)\n",
        "print(\"Rejected Data Directory for Dataset:\", REJECTED_DATASET)\n",
        "\n",
        "print(\"All new directories created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "051c1cae"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to process the main `ZIP_SOURCE_DATASET` which was identified as a directory containing multiple individual ZIP files. This involves copying the contents of this source directory to `RAW_ZIPS_DATASET` first, then iterating through the copied ZIP files, extracting valid ones to `RAW_DATA_DATASET`, and moving any corrupted ZIP files to a designated 'bad_zips' subfolder within `REJECTED_DATASET`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6ab7f07"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define BAD_ZIPS path within REJECTED_DATASET\n",
        "BAD_ZIPS_DATASET = os.path.join(REJECTED_DATASET, \"bad_zips\")\n",
        "os.makedirs(BAD_ZIPS_DATASET, exist_ok=True)\n",
        "\n",
        "print(f\"Processing ZIP source: {ZIP_SOURCE_DATASET}\")\n",
        "\n",
        "# First, copy the contents of the ZIP_SOURCE_DATASET directory to RAW_ZIPS_DATASET\n",
        "# This handles the case where ZIP_SOURCE_DATASET is a directory of zips\n",
        "if os.path.isdir(ZIP_SOURCE_DATASET):\n",
        "    print(f\"'{ZIP_SOURCE_DATASET}' is a directory. Copying its contents to '{RAW_ZIPS_DATASET}'.\")\n",
        "\n",
        "    # Clear existing contents of RAW_ZIPS_DATASET to avoid conflicts if rerunning\n",
        "    if os.path.exists(RAW_ZIPS_DATASET) and os.listdir(RAW_ZIPS_DATASET):\n",
        "        print(f\"Clearing existing contents of '{RAW_ZIPS_DATASET}' before copying.\")\n",
        "        for item in os.listdir(RAW_ZIPS_DATASET):\n",
        "            item_path = os.path.join(RAW_ZIPS_DATASET, item)\n",
        "            if os.path.isdir(item_path):\n",
        "                shutil.rmtree(item_path)\n",
        "            else:\n",
        "                os.remove(item_path)\n",
        "\n",
        "    # Copy items from ZIP_SOURCE_DATASET to RAW_ZIPS_DATASET\n",
        "    for item_name in os.listdir(ZIP_SOURCE_DATASET):\n",
        "        src_item_path = os.path.join(ZIP_SOURCE_DATASET, item_name)\n",
        "        dest_item_path = os.path.join(RAW_ZIPS_DATASET, item_name)\n",
        "        if os.path.isdir(src_item_path):\n",
        "            shutil.copytree(src_item_path, dest_item_path, dirs_exist_ok=True)\n",
        "        else:\n",
        "            shutil.copy2(src_item_path, dest_item_path)\n",
        "    print(\"Contents copy complete.\")\n",
        "elif os.path.isfile(ZIP_SOURCE_DATASET):\n",
        "    # This case is less likely given previous output, but included for robustness\n",
        "    # If ZIP_SOURCE_DATASET was a single zip file, copy it to RAW_ZIPS_DATASET\n",
        "    zip_filename = os.path.basename(ZIP_SOURCE_DATASET)\n",
        "    DEST_ZIP_PATH = os.path.join(RAW_ZIPS_DATASET, zip_filename)\n",
        "    print(f\"Copying single ZIP file '{ZIP_SOURCE_DATASET}' to '{DEST_ZIP_PATH}'...\")\n",
        "    shutil.copy(ZIP_SOURCE_DATASET, DEST_ZIP_PATH)\n",
        "    print(\"Copy complete.\")\n",
        "else:\n",
        "    raise ValueError(f\"Error: '{ZIP_SOURCE_DATASET}' is neither a file nor a directory. Unexpected type.\")\n",
        "\n",
        "\n",
        "good, bad = 0, 0\n",
        "\n",
        "print(f\"Extracting ZIP files from '{RAW_ZIPS_DATASET}' to '{RAW_DATA_DATASET}'...\")\n",
        "for file in os.listdir(RAW_ZIPS_DATASET):\n",
        "    if file.lower().endswith(\".zip\"):\n",
        "        zip_path = os.path.join(RAW_ZIPS_DATASET, file)\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(RAW_DATA_DATASET)\n",
        "                good += 1\n",
        "        except zipfile.BadZipFile:\n",
        "            # Move bad zip for record\n",
        "            shutil.move(zip_path, os.path.join(BAD_ZIPS_DATASET, file))\n",
        "            bad += 1\n",
        "\n",
        "print(f\"Valid ZIPs extracted: {good}\")\n",
        "print(f\"Bad / corrupted ZIPs moved: {bad}\")\n",
        "\n",
        "print(\"First 5 items in RAW_DATA_DATASET after extraction:\")\n",
        "for item in os.listdir(RAW_DATA_DATASET)[:5]:\n",
        "    print(item)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa61f96b"
      },
      "source": [
        "# Task\n",
        "The execution of the previous cell was interrupted. I will re-run the cell to complete the extraction of the ZIP files.\n",
        "\n",
        "```python\n",
        "import shutil\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define BAD_ZIPS path within REJECTED_DATASET\n",
        "BAD_ZIPS_DATASET = os.path.join(REJECTED_DATASET, \"bad_zips\")\n",
        "os.makedirs(BAD_ZIPS_DATASET, exist_ok=True)\n",
        "\n",
        "print(f\"Processing ZIP source: {ZIP_SOURCE_DATASET}\")\n",
        "\n",
        "# First, copy the contents of the ZIP_SOURCE_DATASET directory to RAW_ZIPS_DATASET\n",
        "# This handles the case where ZIP_SOURCE_DATASET is a directory of zips\n",
        "if os.path.isdir(ZIP_SOURCE_DATASET):\n",
        "    print(f\"'{ZIP_SOURCE_DATASET}' is a directory. Copying its contents to '{RAW_ZIPS_DATASET}'.\")\n",
        "\n",
        "    # Clear existing contents of RAW_ZIPS_DATASET to avoid conflicts if rerunning\n",
        "    if os.path.exists(RAW_ZIPS_DATASET) and os.listdir(RAW_ZIPS_DATASET):\n",
        "        print(f\"Clearing existing contents of '{RAW_ZIPS_DATASET}' before copying.\")\n",
        "        for item in os.listdir(RAW_ZIPS_DATASET):\n",
        "            item_path = os.path.join(RAW_ZIPS_DATASET, item)\n",
        "            if os.path.isdir(item_path):\n",
        "                shutil.rmtree(item_path)\n",
        "            else:\n",
        "                os.remove(item_path)\n",
        "\n",
        "    # Copy items from ZIP_SOURCE_DATASET to RAW_ZIPS_DATASET\n",
        "    for item_name in os.listdir(ZIP_SOURCE_DATASET):\n",
        "        src_item_path = os.path.join(ZIP_SOURCE_DATASET, item_name)\n",
        "        dest_item_path = os.path.join(RAW_ZIPS_DATASET, item_name)\n",
        "        if os.path.isdir(src_item_path):\n",
        "            shutil.copytree(src_item_path, dest_item_path, dirs_exist_ok=True)\n",
        "        else:\n",
        "            shutil.copy2(src_item_path, dest_item_path)\n",
        "    print(\"Contents copy complete.\")\n",
        "elif os.path.isfile(ZIP_SOURCE_DATASET):\n",
        "    # This case is less likely given previous output, but included for robustness\n",
        "    # If ZIP_SOURCE_DATASET was a single zip file, copy it to RAW_ZIPS_DATASET\n",
        "    zip_filename = os.path.basename(ZIP_SOURCE_DATASET)\n",
        "    DEST_ZIP_PATH = os.path.join(RAW_ZIPS_DATASET, zip_filename)\n",
        "    print(f\"Copying single ZIP file '{ZIP_SOURCE_DATASET}' to '{DEST_ZIP_PATH}'...\")\n",
        "    shutil.copy(ZIP_SOURCE_DATASET, DEST_ZIP_PATH)\n",
        "    print(\"Copy complete.\")\n",
        "else:\n",
        "    raise ValueError(f\"Error: '{ZIP_SOURCE_DATASET}' is neither a file nor a directory. Unexpected type.\")\n",
        "\n",
        "\n",
        "good, bad = 0, 0\n",
        "\n",
        "print(f\"Extracting ZIP files from '{RAW_ZIPS_DATASET}' to '{RAW_DATA_DATASET}'...\")\n",
        "for file in os.listdir(RAW_ZIPS_DATASET):\n",
        "    if file.lower().endswith(\".zip\"):\n",
        "        zip_path = os.path.join(RAW_ZIPS_DATASET, file)\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(RAW_DATA_DATASET)\n",
        "                good += 1\n",
        "        except zipfile.BadZipFile:\n",
        "            # Move bad zip for record\n",
        "            shutil.move(zip_path, os.path.join(BAD_ZIPS_DATASET, file))\n",
        "            bad += 1\n",
        "\n",
        "print(f\"Valid ZIPs extracted: {good}\")\n",
        "print(f\"Bad / corrupted ZIPs moved: {bad}\")\n",
        "\n",
        "print(\"First 5 items in RAW_DATA_DATASET after extraction:\")\n",
        "for item in os.listdir(RAW_DATA_DATASET)[:5]:\n",
        "    print(item)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a428089"
      },
      "source": [
        "## Extract ZIP Files from New Raw Zips\n",
        "\n",
        "### Subtask:\n",
        "Unzip all '.zip' files found within the new raw zips directory into the new raw data folder. Corrupted ZIPs will be moved to a 'bad_zips' subfolder in the new rejected data directory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e5e1178"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution was interrupted during the ZIP file extraction. I will regenerate the complete code block to ensure all ZIP files are extracted from the `RAW_ZIPS_DATASET` into `RAW_DATA_DATASET`, handling any `BadZipFile` errors by moving them to `BAD_ZIPS_DATASET`, and then verifying the outcome.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a7e93c2"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define BAD_ZIPS path within REJECTED_DATASET\n",
        "BAD_ZIPS_DATASET = os.path.join(REJECTED_DATASET, \"bad_zips\")\n",
        "os.makedirs(BAD_ZIPS_DATASET, exist_ok=True)\n",
        "\n",
        "print(f\"Processing ZIP source: {ZIP_SOURCE_DATASET}\")\n",
        "\n",
        "# First, copy the contents of the ZIP_SOURCE_DATASET directory to RAW_ZIPS_DATASET\n",
        "# This handles the case where ZIP_SOURCE_DATASET is a directory of zips\n",
        "if os.path.isdir(ZIP_SOURCE_DATASET):\n",
        "    print(f\"'{ZIP_SOURCE_DATASET}' is a directory. Copying its contents to '{RAW_ZIPS_DATASET}'.\")\n",
        "\n",
        "    # Clear existing contents of RAW_ZIPS_DATASET to avoid conflicts if rerunning\n",
        "    if os.path.exists(RAW_ZIPS_DATASET) and os.listdir(RAW_ZIPS_DATASET):\n",
        "        print(f\"Clearing existing contents of '{RAW_ZIPS_DATASET}' before copying.\")\n",
        "        for item in os.listdir(RAW_ZIPS_DATASET):\n",
        "            item_path = os.path.join(RAW_ZIPS_DATASET, item)\n",
        "            if os.path.isdir(item_path):\n",
        "                shutil.rmtree(item_path)\n",
        "            else:\n",
        "                os.remove(item_path)\n",
        "\n",
        "    # Copy items from ZIP_SOURCE_DATASET to RAW_ZIPS_DATASET\n",
        "    for item_name in os.listdir(ZIP_SOURCE_DATASET):\n",
        "        src_item_path = os.path.join(ZIP_SOURCE_DATASET, item_name)\n",
        "        dest_item_path = os.path.join(RAW_ZIPS_DATASET, item_name)\n",
        "        if os.path.isdir(src_item_path):\n",
        "            shutil.copytree(src_item_path, dest_item_path, dirs_exist_ok=True)\n",
        "        else:\n",
        "            shutil.copy2(src_item_path, dest_item_path)\n",
        "    print(\"Contents copy complete.\")\n",
        "elif os.path.isfile(ZIP_SOURCE_DATASET):\n",
        "    # This case is less likely given previous output, but included for robustness\n",
        "    # If ZIP_SOURCE_DATASET was a single zip file, copy it to RAW_ZIPS_DATASET\n",
        "    zip_filename = os.path.basename(ZIP_SOURCE_DATASET)\n",
        "    DEST_ZIP_PATH = os.path.join(RAW_ZIPS_DATASET, zip_filename)\n",
        "    print(f\"Copying single ZIP file '{ZIP_SOURCE_DATASET}' to '{DEST_ZIP_PATH}'...\")\n",
        "    shutil.copy(ZIP_SOURCE_DATASET, DEST_ZIP_PATH)\n",
        "    print(\"Copy complete.\")\n",
        "else:\n",
        "    raise ValueError(f\"Error: '{ZIP_SOURCE_DATASET}' is neither a file nor a directory. Unexpected type.\")\n",
        "\n",
        "\n",
        "good, bad = 0, 0\n",
        "\n",
        "print(f\"Extracting ZIP files from '{RAW_ZIPS_DATASET}' to '{RAW_DATA_DATASET}'...\")\n",
        "for file in os.listdir(RAW_ZIPS_DATASET):\n",
        "    if file.lower().endswith(\".zip\"):\n",
        "        zip_path = os.path.join(RAW_ZIPS_DATASET, file)\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(RAW_DATA_DATASET)\n",
        "                good += 1\n",
        "        except zipfile.BadZipFile:\n",
        "            # Move bad zip for record\n",
        "            shutil.move(zip_path, os.path.join(BAD_ZIPS_DATASET, file))\n",
        "            bad += 1\n",
        "\n",
        "print(f\"Valid ZIPs extracted: {good}\")\n",
        "print(f\"Bad / corrupted ZIPs moved: {bad}\")\n",
        "\n",
        "print(\"First 5 items in RAW_DATA_DATASET after extraction:\")\n",
        "for item in os.listdir(RAW_DATA_DATASET)[:5]:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9ab554e"
      },
      "source": [
        "## Normalize Images from New Raw Data\n",
        "\n",
        "### Subtask:\n",
        "Organize and copy individual image files from the new raw data folder into a new normalized data directory. Images will be grouped into subfolders based on their inferred identity (either from the parent folder or filename prefix).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67aad4c7"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires organizing and copying image files from the raw data directory to the normalized data directory, grouping them by inferred identity. This involves iterating through files, checking extensions, determining identity, creating destination folders, and copying files, while keeping track of the count.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7ce7384"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. Define a list of common image file extensions\n",
        "IMG_EXTS = (\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".tiff\")\n",
        "\n",
        "# 2. Initialize a counter for copied images\n",
        "copied_images_count = 0\n",
        "\n",
        "# Get the list of top-level items in RAW_DATA_DATASET for tqdm progress bar\n",
        "raw_data_items = os.listdir(RAW_DATA_DATASET)\n",
        "\n",
        "print(f\"Starting normalization from {RAW_DATA_DATASET} to {NORMALIZED_DATASET}\")\n",
        "\n",
        "# 3. Traverse through the RAW_DATA_DATASET directory\n",
        "for root, dirs, files in tqdm(os.walk(RAW_DATA_DATASET), desc=\"Normalizing images\"): # Added tqdm for progress tracking\n",
        "    for file in files:\n",
        "        # 4. For each file encountered, check if its extension matches any in the predefined image extensions list\n",
        "        if file.lower().endswith(IMG_EXTS):\n",
        "            src_path = os.path.join(root, file)\n",
        "\n",
        "            # 5. Determine the identity for each image\n",
        "            parent_folder = Path(root).name\n",
        "            identity = None\n",
        "\n",
        "            # If the image is in a subfolder (not directly in RAW_DATA_DATASET)\n",
        "            if root != RAW_DATA_DATASET:\n",
        "                identity = parent_folder\n",
        "            else:\n",
        "                # If the image is directly in RAW_DATA_DATASET, extract identity from filename\n",
        "                # E.g., '23BTRCL217-01-AN-01.jpg' -> '23BTRCL217'\n",
        "                parts = file.split('-')\n",
        "                if len(parts) > 1 and parts[0].isalnum(): # Basic check for valid identity prefix\n",
        "                    identity = parts[0]\n",
        "                else:\n",
        "                    # Fallback if filename parsing fails, use a generic 'unknown' or skip\n",
        "                    # For this task, we'll try to use the full filename as identity if no clear prefix\n",
        "                    identity = os.path.splitext(file)[0] # Use filename without extension as identity\n",
        "\n",
        "            if identity:\n",
        "                # Clean up identity name (remove special characters, spaces if any)\n",
        "                identity = identity.replace(' ', '_').replace('-', '_').replace('.', '_')\n",
        "\n",
        "                # 6. Create a destination directory for the identified person within NORMALIZED_DATASET\n",
        "                dest_dir = os.path.join(NORMALIZED_DATASET, identity)\n",
        "                os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "                # 7. Copy the image to its designated identity folder\n",
        "                dest_path = os.path.join(dest_dir, file)\n",
        "                if not os.path.exists(dest_path):\n",
        "                    try:\n",
        "                        shutil.copy2(src_path, dest_path) # copy2 preserves metadata\n",
        "                        copied_images_count += 1\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error copying {src_path} to {dest_path}: {e}\")\n",
        "\n",
        "# 9. After processing all files, print the total number of images copied\n",
        "print(f\"Normalization complete. Total images copied: {copied_images_count}\")\n",
        "\n",
        "print(f\"First 5 identities in {NORMALIZED_DATASET}:\")\n",
        "for item in os.listdir(NORMALIZED_DATASET)[:5]:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "005c8dd0"
      },
      "source": [
        "# Task\n",
        "## Normalize Images from New Raw Data\n",
        "\n",
        "### Subtask:\n",
        "Organize and copy individual image files from the new raw data folder into a new normalized data directory. Images will be grouped into subfolders based on their inferred identity (either from the parent folder or filename prefix)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "565674fb"
      },
      "source": [
        "## Normalize Images from New Raw Data\n",
        "\n",
        "### Subtask:\n",
        "Organize and copy individual image files from the new raw data folder into a new normalized data directory. Images will be grouped into subfolders based on their inferred identity (either from the parent folder or filename prefix).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cb2ce65"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block for normalizing images was interrupted. I need to re-execute the same code to ensure all images are properly organized and copied from the raw data directory to the normalized data directory, following the specified identity extraction and directory creation logic, and tracking the count of copied images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6213576c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. Define a list of common image file extensions\n",
        "IMG_EXTS = (\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".tiff\")\n",
        "\n",
        "# 2. Initialize a counter for copied images\n",
        "copied_images_count = 0\n",
        "\n",
        "# Get the list of top-level items in RAW_DATA_DATASET for tqdm progress bar\n",
        "raw_data_items = os.listdir(RAW_DATA_DATASET)\n",
        "\n",
        "print(f\"Starting normalization from {RAW_DATA_DATASET} to {NORMALIZED_DATASET}\")\n",
        "\n",
        "# 3. Traverse through the RAW_DATA_DATASET directory\n",
        "for root, dirs, files in tqdm(os.walk(RAW_DATA_DATASET), desc=\"Normalizing images\"): # Added tqdm for progress tracking\n",
        "    for file in files:\n",
        "        # 4. For each file encountered, check if its extension matches any in the predefined image extensions list\n",
        "        if file.lower().endswith(IMG_EXTS):\n",
        "            src_path = os.path.join(root, file)\n",
        "\n",
        "            # 5. Determine the identity for each image\n",
        "            parent_folder = Path(root).name\n",
        "            identity = None\n",
        "\n",
        "            # If the image is in a subfolder (not directly in RAW_DATA_DATASET)\n",
        "            if root != RAW_DATA_DATASET:\n",
        "                identity = parent_folder\n",
        "            else:\n",
        "                # If the image is directly in RAW_DATA_DATASET, extract identity from filename\n",
        "                # E.g., '23BTRCL217-01-AN-01.jpg' -> '23BTRCL217'\n",
        "                parts = file.split('-')\n",
        "                if len(parts) > 1 and parts[0].isalnum(): # Basic check for valid identity prefix\n",
        "                    identity = parts[0]\n",
        "                else:\n",
        "                    # Fallback if filename parsing fails, use a generic 'unknown' or skip\n",
        "                    # For this task, we'll try to use the full filename without extension as identity\n",
        "                    identity = os.path.splitext(file)[0] # Use filename without extension as identity\n",
        "\n",
        "            if identity:\n",
        "                # Clean up identity name (remove special characters, spaces if any)\n",
        "                identity = identity.replace(' ', '_').replace('-', '_').replace('.', '_')\n",
        "\n",
        "                # 6. Create a destination directory for the identified person within NORMALIZED_DATASET\n",
        "                dest_dir = os.path.join(NORMALIZED_DATASET, identity)\n",
        "                os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "                # 7. Copy the image to its designated identity folder\n",
        "                dest_path = os.path.join(dest_dir, file)\n",
        "                if not os.path.exists(dest_path):\n",
        "                    try:\n",
        "                        shutil.copy2(src_path, dest_path) # copy2 preserves metadata\n",
        "                        copied_images_count += 1\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error copying {src_path} to {dest_path}: {e}\")\n",
        "\n",
        "# 9. After processing all files, print the total number of images copied\n",
        "print(f\"Normalization complete. Total images copied: {copied_images_count}\")\n",
        "\n",
        "print(f\"First 5 identities in {NORMALIZED_DATASET}:\")\n",
        "for item in os.listdir(NORMALIZED_DATASET)[:5]:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "214cab0c"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires setting up subfolders within the new rejected data directory to categorize images that fail subsequent processing steps based on different rejection reasons.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8964dcc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define rejection reasons as per task description\n",
        "REASONS = [\n",
        "    \"corrupt\",\n",
        "    \"no_face\",\n",
        "    \"multiple_faces\",\n",
        "    \"blur\",\n",
        "    \"bad_lighting\"\n",
        "]\n",
        "\n",
        "# Create subfolders for each rejection reason within REJECTED_DATASET\n",
        "for r in REASONS:\n",
        "    os.makedirs(f\"{REJECTED_DATASET}/{r}\", exist_ok=True)\n",
        "\n",
        "print(\"Rejection folders ready within REJECTED_DATASET.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2399dc45"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to filter images from the `NORMALIZED_DATASET` based on face detection, blur, and lighting conditions using relaxed criteria, and move valid images to `CLEAN_DATASET` and rejected ones to specific subfolders within `REJECTED_DATASET`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa328345"
      },
      "outputs": [],
      "source": [
        "import os, shutil, cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Ensure the Haar cascade classifier is loaded\n",
        "# This assumes 'haar' was loaded in a previous cell, if not, re-load it.\n",
        "if 'haar' not in locals():\n",
        "    haar = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "\n",
        "# Define filtering criteria with relaxed values\n",
        "MIN_NEIGHBORS = 3\n",
        "BLUR_THRESHOLD = 50\n",
        "DARK_THRESHOLD = 20\n",
        "BRIGHT_THRESHOLD = 235\n",
        "MAX_IMAGES_PER_ID = 50\n",
        "\n",
        "stats = {\n",
        "    \"total\": 0,\n",
        "    \"kept\": 0,\n",
        "    \"no_face\": 0,\n",
        "    \"blur\": 0,\n",
        "    \"bad_lighting\": 0\n",
        "}\n",
        "\n",
        "print(f\"Starting image filtering from {NORMALIZED_DATASET} to {CLEAN_DATASET}\")\n",
        "\n",
        "for identity in tqdm(os.listdir(NORMALIZED_DATASET), desc=\"Filtering images\"): # Added tqdm for progress\n",
        "    src_id_dir = os.path.join(NORMALIZED_DATASET, identity)\n",
        "    if not os.path.isdir(src_id_dir):\n",
        "        continue\n",
        "\n",
        "    dst_id_dir = os.path.join(CLEAN_DATASET, identity)\n",
        "    os.makedirs(dst_id_dir, exist_ok=True)\n",
        "\n",
        "    kept_for_id = 0\n",
        "\n",
        "    for img_name in os.listdir(src_id_dir):\n",
        "        if kept_for_id >= MAX_IMAGES_PER_ID:\n",
        "            break # Stop processing for this identity if max images are kept\n",
        "\n",
        "        img_path = os.path.join(src_id_dir, img_name)\n",
        "        stats[\"total\"] += 1\n",
        "\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            # Move corrupt images to rejected_data/corrupt\n",
        "            shutil.move(img_path, os.path.join(REJECTED_DATASET, \"corrupt\", img_name))\n",
        "            stats[\"corrupt\"] = stats.get(\"corrupt\", 0) + 1\n",
        "            continue\n",
        "\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Face detection\n",
        "        faces = haar.detectMultiScale(gray, 1.3, MIN_NEIGHBORS)\n",
        "        if len(faces) == 0:\n",
        "            # No face detected, move to rejected_data/no_face\n",
        "            shutil.move(img_path, os.path.join(REJECTED_DATASET, \"no_face\", img_name))\n",
        "            stats[\"no_face\"] += 1\n",
        "            continue\n",
        "        # No 'multiple_faces' rejection for this task, so proceed if faces found\n",
        "\n",
        "        # Blur detection\n",
        "        blur_score = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "        if blur_score < BLUR_THRESHOLD:\n",
        "            # Image is too blurry, move to rejected_data/blur\n",
        "            shutil.move(img_path, os.path.join(REJECTED_DATASET, \"blur\", img_name))\n",
        "            stats[\"blur\"] += 1\n",
        "            continue\n",
        "\n",
        "        # Lighting check\n",
        "        mean_intensity = gray.mean()\n",
        "        if mean_intensity < DARK_THRESHOLD or mean_intensity > BRIGHT_THRESHOLD:\n",
        "            # Image is too dark or too bright, move to rejected_data/bad_lighting\n",
        "            shutil.move(img_path, os.path.join(REJECTED_DATASET, \"bad_lighting\", img_name))\n",
        "            stats[\"bad_lighting\"] += 1\n",
        "            continue\n",
        "\n",
        "        # If all checks pass, copy the image to the clean directory\n",
        "        shutil.copy(img_path, os.path.join(dst_id_dir, img_name))\n",
        "        kept_for_id += 1\n",
        "        stats[\"kept\"] += 1\n",
        "\n",
        "print(\"IMAGE FILTERING COMPLETE\")\n",
        "print(stats)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a54f523c"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to remove filename-based duplicate images from the `CLEAN_DATASET`. This involves iterating through each identity's images, grouping them by emotion (extracted from the filename), and then moving any images exceeding the `MAX_PER_EMOTION` limit (3 as per the task) to a dedicated `duplicates_filename` subfolder within `REJECTED_DATASET`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10d79c89"
      },
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "from collections import defaultdict\n",
        "\n",
        "# Define paths\n",
        "# CLEAN_DATASET is already defined from a previous cell\n",
        "# REJECTED_DATASET is already defined from a previous cell\n",
        "\n",
        "DUP_FN_DATASET = os.path.join(REJECTED_DATASET, \"duplicates_filename\")\n",
        "os.makedirs(DUP_FN_DATASET, exist_ok=True)\n",
        "\n",
        "MAX_PER_EMOTION = 3\n",
        "removed_fn = 0\n",
        "\n",
        "print(f\"Starting filename-based duplicate removal from {CLEAN_DATASET}\")\n",
        "\n",
        "for identity in os.listdir(CLEAN_DATASET):\n",
        "    id_path = os.path.join(CLEAN_DATASET, identity)\n",
        "    if not os.path.isdir(id_path):\n",
        "        continue\n",
        "\n",
        "    groups = defaultdict(list)\n",
        "\n",
        "    for img in os.listdir(id_path):\n",
        "        # Extract emotion from filename (e.g., '23BTRCL217-01-AN-01.jpg' -> 'AN')\n",
        "        # Assuming emotion is the third part after splitting by hyphen, e.g., P_S-ID-EMO-NUM.ext or ID-P_S-EMO-NUM.ext\n",
        "        # Need to handle cases where the filename structure might be slightly different\n",
        "        parts = img.split('-')\n",
        "        emotion = None\n",
        "        if len(parts) >= 3 and len(parts[2]) == 2: # e.g. 'AN', 'DI', 'FE', 'HA', 'NE', 'SA', 'SU'\n",
        "             emotion = parts[2].upper()\n",
        "        elif len(parts) >= 3 and len(parts[1]) == 2: # sometimes emotion is the second part (e.g., ID-EMO-NUM)\n",
        "            emotion = parts[1].upper()\n",
        "        else:\n",
        "            # Fallback for unexpected formats, or if emotion cannot be reliably extracted\n",
        "            # For now, if emotion is ambiguous, we'll treat it as unique (no grouping by emotion)\n",
        "            # or you might define a 'misc' emotion category\n",
        "            pass\n",
        "\n",
        "        if emotion and emotion in ['AN', 'DI', 'FE', 'HA', 'NE', 'SA', 'SU', 'SUR', 'ANGRY', 'DISGUSTING', 'FEAR', 'HAPPY', 'NORMAL', 'SAD', 'SURPRISE']:\n",
        "            groups[emotion].append(img)\n",
        "        else:\n",
        "            # If no identifiable emotion, treat each image as its own group to not accidentally remove\n",
        "            groups[f\"no_emotion_{img}\"].append(img) # Unique key for each non-categorized image\n",
        "\n",
        "    for emotion, imgs in groups.items():\n",
        "        imgs = sorted(imgs)  # Sort to ensure consistent removal (e.g., keep lowest indices)\n",
        "        for extra in imgs[MAX_PER_EMOTION:]:\n",
        "            try:\n",
        "                shutil.move(os.path.join(id_path, extra),\n",
        "                            os.path.join(DUP_FN_DATASET, extra))\n",
        "                removed_fn += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Error moving file {os.path.join(id_path, extra)}: {e}\")\n",
        "\n",
        "print(f\"Filename-based duplicates removed: {removed_fn}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caff2972"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to remove exact pixel duplicates from the `CLEAN_DATASET`. This involves calculating the MD5 hash for each image and moving any images with identical hashes (beyond the first occurrence) to a dedicated `duplicates_exact` subfolder within `REJECTED_DATASET`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1c39bb4"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import os, shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define paths\n",
        "# CLEAN_DATASET is already defined from a previous cell\n",
        "# REJECTED_DATASET is already defined from a previous cell\n",
        "\n",
        "DUP_EXACT_DATASET = os.path.join(REJECTED_DATASET, \"duplicates_exact\")\n",
        "os.makedirs(DUP_EXACT_DATASET, exist_ok=True)\n",
        "\n",
        "hash_seen = {}\n",
        "removed_hash = 0\n",
        "\n",
        "def md5_hash(path):\n",
        "    h = hashlib.md5()\n",
        "    try:\n",
        "        with open(path, \"rb\") as f:\n",
        "            for chunk in iter(lambda: f.read(8192), b\"\"):\n",
        "                h.update(chunk)\n",
        "        return h.hexdigest()\n",
        "    except IOError as e:\n",
        "        print(f\"Error reading file {path}: {e}\")\n",
        "        return None\n",
        "\n",
        "print(f\"Starting exact pixel duplicate removal from {CLEAN_DATASET}\")\n",
        "\n",
        "for identity in tqdm(os.listdir(CLEAN_DATASET), desc=\"Checking for exact pixel duplicates\"):\n",
        "    id_path = os.path.join(CLEAN_DATASET, identity)\n",
        "    if not os.path.isdir(id_path):\n",
        "        continue\n",
        "\n",
        "    # Iterate over a copy of the list of files to avoid issues when moving files\n",
        "    for img in list(os.listdir(id_path)):\n",
        "        img_path = os.path.join(id_path, img)\n",
        "        if not os.path.isfile(img_path):\n",
        "            continue\n",
        "\n",
        "        h = md5_hash(img_path)\n",
        "\n",
        "        if h is None:\n",
        "            # If there was an error reading the file, it's not a valid candidate for comparison\n",
        "            continue\n",
        "\n",
        "        if h in hash_seen:\n",
        "            try:\n",
        "                shutil.move(img_path, os.path.join(DUP_EXACT_DATASET, img))\n",
        "                removed_hash += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Error moving duplicate file {img_path}: {e}\")\n",
        "        else:\n",
        "            hash_seen[h] = img_path\n",
        "\n",
        "print(f\"Exact pixel duplicates removed: {removed_hash}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce75c09b"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that both filename-based and exact pixel duplicates have been removed, the final step in the data cleaning process is to count and report the total number of unique, processed images remaining in the `CLEAN_DATASET` directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a572f9e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "final_count = 0\n",
        "# Iterate through each identity folder in the CLEAN_DATASET\n",
        "for identity_folder in os.listdir(CLEAN_DATASET):\n",
        "    identity_path = os.path.join(CLEAN_DATASET, identity_folder)\n",
        "    if os.path.isdir(identity_path):\n",
        "        # Count the image files within each identity folder\n",
        "        for f in os.listdir(identity_path):\n",
        "            if f.lower().endswith(IMG_EXTS):\n",
        "                final_count += 1\n",
        "\n",
        "print(f\"FINAL UNIQUE KEPT IMAGES: {final_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ad7ec8d"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the entire data processing pipeline for the 'DATASET' shortcut, including the total number of images kept after cleaning and deduplication, and any key observations from the process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fa9c5b8"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### RESULT\n",
        "Total Images are 6017"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}